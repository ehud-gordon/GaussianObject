{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import math\n",
    "import os\n",
    "from argparse import Namespace\n",
    "\n",
    "# Get the home directory\n",
    "home_dir = os.path.expanduser('~')\n",
    "\n",
    "# Set the environment variable, to solve LIBSTDC linker problems\n",
    "os.environ['LD_LIBRARY_PATH'] = f\"{home_dir}/miniconda3/envs/gobj/lib/:\" + os.environ.get('LD_LIBRARY_PATH', '')\n",
    "\n",
    "import camtools as ct\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import torch\n",
    "from tqdm import trange\n",
    "from scene.dataset_readers import sceneLoadTypeCallbacks\n",
    "from utils.camera_utils import cameraList_from_camInfos\n",
    "from torch.nn import functional as F\n",
    "import copy\n",
    "from typing import NamedTuple\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SceneInfo(NamedTuple):\n",
    "    Ks: list\n",
    "    Ts: list\n",
    "    images: list\n",
    "    masks: list\n",
    "    \n",
    "\n",
    "def fov2focal(fov, pixels):\n",
    "    return pixels / (2 * math.tan(fov / 2))\n",
    "\n",
    "def points2homopoints(points):\n",
    "    assert points.shape[-1] == 3\n",
    "    bottom = torch.ones_like(points[...,0:1])\n",
    "    return torch.cat([points, bottom], dim=-1)\n",
    "\n",
    "def batch_projection(Ks, Ts, points):\n",
    "    '''\n",
    "    Ks: B, 3, 3\n",
    "    Ts: B, 4, 4\n",
    "    points: B, N, 3\n",
    "    '''\n",
    "    pre_fix = points.shape[:-1] # [100, 100]\n",
    "    points = points.reshape(-1, 3) # [M, 3]\n",
    "\n",
    "    Ts = torch.stack(Ts, dim=0) # [N, 4, 4]\n",
    "    Ks = torch.stack(Ks, dim=0).to(Ts.device) # [N, 3, 3]\n",
    "    camera_num = Ks.shape[0]\n",
    "    homopts = points2homopoints(points) # [M, 4]\n",
    "    # world to camera # [N, M, 4] @ [N, 4, 4] = [N, M, 4]\n",
    "    homopts_cam = torch.bmm(homopts.unsqueeze(0).repeat_interleave(Ts.shape[0], dim=0), Ts.transpose(1,2)) \n",
    "    # camera to image space  # [N, M, 4] @ [N, 4, 3] = [N, M, 3]\n",
    "    homopts_img = torch.bmm(homopts_cam[...,:3], Ks.transpose(1,2))\n",
    "    # normalize\n",
    "    homopts_img = homopts_img / (homopts_img[...,2:] + 1e-6)\n",
    "    # reshape back\n",
    "    homopts_img = homopts_img.reshape(camera_num, *pre_fix, 3)\n",
    "    homopts_cam = homopts_cam.reshape(camera_num, *pre_fix, 4)\n",
    "    return homopts_img[...,0:2], homopts_cam[...,2]\n",
    "\n",
    "def query_from_list_with_list(listA: list, listB: list):\n",
    "    '''\n",
    "    listA: [1, 2, 3]\n",
    "    listB: [3, 2, 1]\n",
    "    return: [2, 1, 0]\n",
    "    '''\n",
    "    return [listB[i] for i in listA]\n",
    "\n",
    "def simple_resize_image(img, size):\n",
    "    return transforms.Resize(size, antialias=True)(img)\n",
    "\n",
    "def get_visual_hull(N, bbox, scene_info, cam_center):\n",
    "    pcs = []\n",
    "    color = []\n",
    "    all_pts = []\n",
    "    Ks = scene_info.Ks\n",
    "    Ts = scene_info.Ts\n",
    "    images = scene_info.images\n",
    "    masks = scene_info.masks\n",
    "\n",
    "    [xs, ys, zs], [xe, ye, ze] = bbox[0], bbox[1]\n",
    "\n",
    "    # please note that in vasedeck, the images are not same size, for simplify, just resize them\n",
    "    new_images = []\n",
    "    new_masks = []\n",
    "    img_size = images[0].shape[1:]\n",
    "    for image, mask in zip(images, masks):\n",
    "        new_images.append(simple_resize_image(image, img_size))\n",
    "        new_masks.append(simple_resize_image(mask, img_size))\n",
    "\n",
    "    images = torch.stack(new_images) # N C H W\n",
    "    masks = torch.stack(new_masks) # N 1 H W\n",
    "\n",
    "    for h_id in trange(N):\n",
    "        i, j = torch.meshgrid(torch.linspace(xs, xe, N).cuda(),\n",
    "                              torch.linspace(ys, ye, N).cuda())\n",
    "        i, j = i.t(), j.t()\n",
    "        pts = torch.stack([i, j, torch.ones_like(i).cuda()], -1)\n",
    "        pts[...,2] = h_id / N * (ze - zs) + zs # 100, 100, 3\n",
    "\n",
    "        # shift the pts to be centered at the camera center\n",
    "        pts[...,0] += cam_center[0]  # note the order, [x, y, z], width, height, depth\n",
    "        pts[...,1] += cam_center[1]\n",
    "        pts[...,2] += cam_center[2]\n",
    "\n",
    "        all_pts.append(pts)\n",
    "\n",
    "        # now we have the pts, we need to project them to the image plane\n",
    "        # batched projection\n",
    "        uv, z = batch_projection(Ks, Ts, pts) # [N, 100, 100, 2], [N, 100, 100]\n",
    "        valid_z_mask = z > 0\n",
    "        valid_x_y_mask = (uv[...,0] > 0) & (uv[...,0] < cam_info.image_width) & (uv[...,1] > 0) & (uv[...,1] < cam_info.image_height)\n",
    "        valid_pt_mask = valid_z_mask & valid_x_y_mask\n",
    "\n",
    "        # simple resize the uv to [-1, 1]\n",
    "        uv[...,0] = uv[...,0] / cam_info.image_width * 2 - 1\n",
    "        uv[...,1] = uv[...,1] / cam_info.image_height * 2 - 1\n",
    "\n",
    "        # now we have the uv, we use grid_sample to sample the image to get the color\n",
    "        result = F.grid_sample(images.float(), uv, padding_mode='zeros', align_corners=False).permute(0, 2, 3, 1) # N, 100, 100, 3\n",
    "        # sample mask\n",
    "        result_mask = F.grid_sample(masks.float(), uv, padding_mode='zeros', align_corners=False).permute(0, 2, 3, 1) # N, 100, 100, 1\n",
    "\n",
    "        valid_pt_mask = result_mask.squeeze() > 0 & valid_pt_mask\n",
    "\n",
    "        pcs.append(valid_pt_mask.float().sum(0) >= (images.shape[0] - 1)) # [100, 100]\n",
    "        color.append(result.mean(0)) # [100, 100, 3]\n",
    "    \n",
    "    pcs = torch.stack(pcs, -1)\n",
    "    color = torch.stack(color, -1)\n",
    "\n",
    "    r, g, b = color[:, :, 0], color[:, :, 1], color[:, :, 2]\n",
    "    idx = torch.where(pcs > 0)\n",
    "\n",
    "    color = torch.stack((r[idx] * 255, g[idx] * 255, b[idx] * 255), -1)\n",
    "\n",
    "    idx = torch.stack([idx[1], idx[0], idx[2]], -1) # note the order is hwz -> xyz\n",
    "    # turn the idx to the point position used in batch_projection\n",
    "    idx = idx.float() / N\n",
    "    idx[...,0] = idx[...,0] * (xe - xs) + xs + cam_center[0]\n",
    "    idx[...,1] = idx[...,1] * (ye - ys) + ys + cam_center[1]\n",
    "    idx[...,2] = idx[...,2] * (ze - zs) + zs + cam_center[2]\n",
    "\n",
    "    print(\"visual hull is Okay, with {} points\".format(idx.shape[0]))\n",
    "    # we get the point cloud, use open3d to visualize it\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(idx.cpu().numpy())\n",
    "    pcd.colors = o3d.utility.Vector3dVector(color.cpu().numpy() / 255)\n",
    "\n",
    "    # get bbox\n",
    "    bbox = pcd.get_axis_aligned_bounding_box()\n",
    "    return pcd, bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the sparse id is 4, with 4 train frames\n",
      "Using resized images in data/helmet2/images...\n",
      "Reading camera 2/4"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading camera 4/4\n",
      "the sparse id is 4, with 8 test frames\n",
      "Using resized images in data/helmet2/images...\n",
      "Reading camera 8/8\n",
      "Generating ellipse path from 4 camera infos ...\n",
      "theta[0] 0.0\n",
      "the camera center is: tensor([-0.0629, -0.8866, -0.9864], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]/home/ehud.gordon/miniconda3/envs/gobj/lib/python3.8/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403590347/work/aten/src/ATen/native/TensorShape.cpp:3549.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1248.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visual hull is Okay, with 99783 points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 64/64 [00:00<00:00, 1732.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visual hull is Okay, with 20035 points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='generate k views covering object')\n",
    "parser.add_argument('--data_dir', type=str, default='data/helmet2', help='data directory, we only support colmap type data, kitchen, garden')\n",
    "parser.add_argument(\"--cube_size\", type=float, default=4.0, help=\"size of the cube in meters\")\n",
    "parser.add_argument(\"--voxel_num\", type=int, default=200, help=\"size of a voxel in meters\")\n",
    "parser.add_argument('--sparse_view_num', type=int, default=4, help='sparse id')\n",
    "parser.add_argument('--reso', type=int, default=1, help='the resolution of image, 1 for omni3d, 4 or 8 for mip360')\n",
    "parser.add_argument('--not_vis', action='store_true', help='whether vis the visual hull, is enable, not vis')\n",
    "parser.add_argument(\"--cube_size_shift_x\", type=float, default=0.0, help=\"shift sizex of the cube in meters\")\n",
    "parser.add_argument(\"--cube_size_shift_y\", type=float, default=0.0, help=\"shift sizey of the cube in meters\")\n",
    "parser.add_argument(\"--cube_size_shift_z\", type=float, default=0.0, help=\"shift sizez of the cube in meters\")\n",
    "args = parser.parse_args([])\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "extra_opts = Namespace()\n",
    "extra_opts.sparse_view_num = -1\n",
    "extra_opts.resolution = args.reso\n",
    "extra_opts.use_mask = True\n",
    "extra_opts.data_device = 'cuda'\n",
    "extra_opts.init_pcd_name = 'origin'\n",
    "extra_opts.white_background = False\n",
    "extra_opts.sparse_view_num = args.sparse_view_num   \n",
    "\n",
    "# load the camera parameters\n",
    "# we assume that the camera parameters are stored in the data_dir\n",
    "scene_info = sceneLoadTypeCallbacks[\"Colmap\"](args.data_dir, 'images', False, extra_opts=extra_opts) \n",
    "camlist = cameraList_from_camInfos(scene_info.train_cameras, 1.0, extra_opts)\n",
    "\n",
    "selected_id = np.arange(len(camlist))\n",
    "\n",
    "# get all camera locations to recenter the scene\n",
    "cam_locations = []\n",
    "cam_rotations = []\n",
    "cam_T = []\n",
    "Ts = []\n",
    "Ks = []\n",
    "images = []\n",
    "masks = []\n",
    "for cam_info in camlist:\n",
    "    cam_locations.append(cam_info.camera_center)\n",
    "    cam_rotations.append(cam_info.R)\n",
    "    cam_T.append(cam_info.T)\n",
    "    Ts.append(cam_info.world_view_transform.T)\n",
    "    fx = fov2focal(cam_info.FoVx, cam_info.image_width)\n",
    "    fy = fov2focal(cam_info.FoVy, cam_info.image_height)\n",
    "    Ks.append(torch.tensor([[fx, 0, cam_info.image_width/2], [0, fy, cam_info.image_height/2], [0, 0, 1]]))\n",
    "    images.append(cam_info.original_image)\n",
    "    masks.append(cam_info.mask)\n",
    "\n",
    "# in this time, we already have the camera parameters\n",
    "# first, we get the cemera locations center\n",
    "cam_center = torch.stack(cam_locations).mean(0)\n",
    "print('the camera center is:', cam_center)\n",
    "\n",
    "Ks = query_from_list_with_list(selected_id, Ks)\n",
    "Ts = query_from_list_with_list(selected_id, Ts)\n",
    "images = query_from_list_with_list(selected_id, images)\n",
    "masks = query_from_list_with_list(selected_id, masks)\n",
    "\n",
    "scene_info = SceneInfo(Ks, Ts, images, masks)\n",
    "Ks_clone = copy.deepcopy(Ks)\n",
    "\n",
    "bx = args.cube_size\n",
    "init_bbox = [[args.cube_size_shift_x-bx, args.cube_size_shift_y-bx, args.cube_size_shift_z-bx], \n",
    "                [args.cube_size_shift_x+bx, args.cube_size_shift_y+bx, args.cube_size_shift_z+bx]]\n",
    "# we run the get_visual_hull twice, first to get the bound, second to get the visual hull\n",
    "pcd, bbox = get_visual_hull(args.voxel_num, init_bbox, scene_info, cam_center)\n",
    "\n",
    "# since we get the bound, we use this bound to better recon\n",
    "# we use the center of the bound as the center of the scene\n",
    "# please note that the bbox may need bigger, since the camera may not cover the whole scene\n",
    "bbox_min = bbox.get_min_bound()\n",
    "bbox_max = bbox.get_max_bound()\n",
    "# Calculate the center point of the original bounding box\n",
    "center = (bbox_min + bbox_max) / 2\n",
    "# Calculate the extents of the original bounding box\n",
    "extents = bbox_max - bbox_min\n",
    "# Calculate the scale factor to increase the size by 20% (1.2 times)\n",
    "scale_factor = 2\n",
    "# Calculate the scaled extents\n",
    "scaled_extents = extents * scale_factor\n",
    "# Calculate the new minimum and maximum points of the enlarged bounding box\n",
    "enlarged_bbox_min = center - scaled_extents / 2\n",
    "enlarged_bbox_max = center + scaled_extents / 2\n",
    "\n",
    "pcd, bbox_new = get_visual_hull(64, [enlarged_bbox_min, enlarged_bbox_max], scene_info, [0,0,0])\n",
    "# save the pointcloud\n",
    "if args.sparse_view_num >= 0:\n",
    "    o3d.io.write_point_cloud(os.path.join(args.data_dir, f\"visual_hull_{str(args.sparse_view_num)}.ply\"), pcd)\n",
    "else:\n",
    "    o3d.io.write_point_cloud(os.path.join(args.data_dir, \"visual_hull_full.ply\"), pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Open3D INFO] Resetting default logger to print to terminal.\n"
     ]
    }
   ],
   "source": [
    "from open3d.web_visualizer import draw\n",
    "Ts = np.array([i.cpu().numpy() for i in Ts])\n",
    "Ks = np.array([k.cpu().numpy() for k in Ks_clone])\n",
    "cameras = ct.camera.create_camera_frames(Ks, Ts, highlight_color_map={0: [1, 0, 0], -1: [0, 1, 0]})\n",
    "# build LineSet to represent the coordinate\n",
    "world_coord = o3d.geometry.LineSet()\n",
    "world_coord.points = o3d.utility.Vector3dVector(np.array([[0, 0, 0], [2, 0, 0], \n",
    "                                                        [0, 0, 0], [0, 2, 0], \n",
    "                                                        [0, 0, 0], [0, 0, 2]]))\n",
    "world_coord.lines = o3d.utility.Vector2iVector(np.array([[0, 1], [0, 3], [0, 5]]))\n",
    "# X->red, Y->green, Z->blue\n",
    "world_coord.colors = o3d.utility.Vector3dVector(np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]))\n",
    "\n",
    "pcdo = o3d.io.read_point_cloud(os.path.join(args.data_dir, \"sparse/0/points3D.ply\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Open3D INFO] Window window_0 created.\n",
      "[Open3D INFO] EGL headless mode enabled.\n",
      "[Open3D INFO] ICE servers: [\"stun:stun.l.google.com:19302\", \"turn:user:password@34.69.27.100:3478\", \"turn:user:password@34.69.27.100:3478?transport=tcp\"]\n",
      "FEngine (64 bits) created at 0x7f4d2400a1a0 (threading is enabled)\n",
      "[Open3D INFO] Set WEBRTC_STUN_SERVER environment variable add a customized WebRTC STUN server.\n",
      "[Open3D INFO] WebRTC Jupyter handshake mode enabled.\n",
      "EGL(1.5)\n",
      "OpenGL(4.1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc9cbe8b86743f788f4e3f26d2c0564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "WebVisualizer(window_uid='window_0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Open3D INFO] Sending init frames to window_0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[000:000][1236147] (stun_port.cc:96): Binding request timed out from 132.68.39.x:60206 (enp225s0f0)\n",
      "[000:005][1236147] (stun_port.cc:96): Binding request timed out from 132.68.39.x:60206 (enp225s0f0)\n"
     ]
    }
   ],
   "source": [
    "draw([world_coord, pcd, cameras])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gobj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
